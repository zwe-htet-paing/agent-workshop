{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c24091",
   "metadata": {},
   "source": [
    "# Generative AI vs Agentic AI\n",
    "\n",
    "### Generative AI\n",
    "- Generative AI creates content such as text, images, or code. \n",
    "- It is **reactive**, meaning it responds directly to prompts from the user.  \n",
    "**Example:** ChatGPT writing a blog or generating a summary.\n",
    "\n",
    "### Agentic AI\n",
    "- Agentic AI performs tasks **autonomously** to achieve specific goals. \n",
    "- It is **proactive**, capable of planning, using tools, and executing actions without step-by-step instructions.  \n",
    "**Example:** AutoGPT researching information and booking a trip.\n",
    "\n",
    "### Key Difference\n",
    "- **Generative AI:** content creator  \n",
    "- **Agentic AI:** autonomous actor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74477d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "load_dotenv()\n",
    "# os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c50f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1d8c4e",
   "metadata": {},
   "source": [
    "### Generative AI Example (Chat completion)\n",
    "\n",
    "- Question → Final Answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "802280ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeb11c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIMessage(content='LangChain is a framework designed to facilitate the development of applications powered by large language models (LLMs). It provides a structured way to integrate LLMs with various data sources, APIs, and tools, enabling developers to create more complex and interactive applications. LangChain supports features such as prompt management, memory, and chaining of multiple LLM calls, allowing for sophisticated workflows and enhanced user experiences. Its modular architecture makes it adaptable for various use cases, including chatbots, data analysis, and content generation, making it a valuable resource for developers looking to leverage the capabilities of LLMs effectively.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 119, 'prompt_tokens': 15, 'total_tokens': 134, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'id': 'chatcmpl-CBJu3zKweUirqx1ToL5iGM2l3a6eX', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--dde0d37c-414f-4d58-9ffc-fa26b26257ca-0', usage_metadata={'input_tokens': 15, 'output_tokens': 119, 'total_tokens': 134, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a short summary about LangChain.\"\n",
    "response = llm.invoke([HumanMessage(content=prompt)])\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24b3b697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 119,\n",
       "  'prompt_tokens': 15,\n",
       "  'total_tokens': 134,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-4o-mini-2024-07-18',\n",
       " 'system_fingerprint': 'fp_8bda4d3a2c',\n",
       " 'id': 'chatcmpl-CBJu3zKweUirqx1ToL5iGM2l3a6eX',\n",
       " 'service_tier': 'default',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c1fc9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain is a framework designed to facilitate the development of applications powered by large language models (LLMs). It provides a structured way to integrate LLMs with various data sources, APIs, and tools, enabling developers to create more complex and interactive applications. LangChain supports features such as prompt management, memory, and chaining of multiple LLM calls, allowing for sophisticated workflows and enhanced user experiences. Its modular architecture makes it adaptable for various use cases, including chatbots, data analysis, and content generation, making it a valuable resource for developers looking to leverage the capabilities of LLMs effectively."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22badcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework designed to facilitate the development of applications that leverage large language models (LLMs). It provides a structured way to integrate LLMs with various data sources, APIs, and tools, enabling developers to create more complex and interactive applications. LangChain supports features like prompt management, memory, and chaining of multiple LLM calls, allowing for sophisticated workflows and enhanced user experiences. Its modular architecture makes it adaptable for various use cases, including chatbots, data analysis, and content generation, making it a valuable resource for developers working with AI-driven solutions."
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream([HumanMessage(content=prompt)]):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd12419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc86e7c9",
   "metadata": {},
   "source": [
    "### Agentic AI Example (Agent using tools)\n",
    "\n",
    "- Question → Reasoning → Action → Observation → Updated Reasoning → Final Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c832e263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_130963/1608596962.py:20: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to perform the multiplication of 2 and 3.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"2 * 3\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m6\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: 6\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'What is 2 * 3?', 'output': '6'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "\n",
    "\n",
    "def calculator(query: str) -> str:\n",
    "    try:\n",
    "        return str(eval(query))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "calculator_tool = Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=calculator,\n",
    "        description=\"Perform simple arithmetic calculations.\",\n",
    "    )\n",
    "\n",
    "tools = [calculator_tool] \n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    \n",
    "# Ask the agent a question\n",
    "result = agent.invoke(\"What is 2 * 3?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc44635c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a18b539",
   "metadata": {},
   "source": [
    "### Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16858b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful assistant.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "So you said you were researching ocean mammals?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Oliver\n",
      "\n",
      "Yes, that's right.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: Model\n",
      "\n",
      "Great, what would you like to learn about.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Oliver\n",
      "\n",
      "I want to learn about the best place to see Orcas in the US.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [SystemMessage(content=\"You are a helpful assistant.\")]\n",
    "messages.append(AIMessage(content=f\"So you said you were researching ocean mammals?\", name=\"Model\"))\n",
    "messages.append(HumanMessage(content=f\"Yes, that's right.\",name=\"Oliver\"))\n",
    "messages.append(AIMessage(content=f\"Great, what would you like to learn about.\", name=\"Model\"))\n",
    "messages.append(HumanMessage(content=f\"I want to learn about the best place to see Orcas in the US.\", name=\"Oliver\"))\n",
    "\n",
    "for m in messages:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0749b2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"One of the best places to see orcas in the U.S. is the Pacific Northwest, particularly around the San Juan Islands in Washington State. The waters surrounding these islands are home to resident orca pods, especially the Southern Resident Killer Whales, which are often spotted from late spring through early fall.\\n\\nAnother excellent location is the waters off the coast of Alaska, where transient orcas can be seen hunting for seals and other marine mammals. Places like Kenai Fjords National Park and Glacier Bay National Park are popular for orca watching.\\n\\nAdditionally, you can also find orcas in the waters of California, particularly around Monterey Bay, where they are sometimes seen during the summer months.\\n\\nIf you're planning a trip, consider going on a whale-watching tour, as local guides can provide the best chances of spotting these magnificent creatures.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 166, 'prompt_tokens': 77, 'total_tokens': 243, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CBKE9LlyyIbZ2Y9lJq3YauCuR4H2Y', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--de6cb216-077c-462f-9484-f4b44dc74b4c-0', usage_metadata={'input_tokens': 77, 'output_tokens': 166, 'total_tokens': 243, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e78b5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "One of the best places to see orcas in the U.S. is the San Juan Islands in Washington State. The waters around these islands are known for their resident orca pods, particularly the Southern Resident Killer Whales. The best time to see them is typically from late spring to early fall, especially in June through September.\n",
       "\n",
       "Another excellent location is the waters off the coast of Alaska, where you can find transient orcas that hunt for seals and other marine mammals. Places like Kenai Fjords National Park and Glacier Bay National Park are popular for orca sightings.\n",
       "\n",
       "Additionally, the waters around Monterey Bay in California can also be a good spot to see orcas, especially during the migration seasons when they follow their prey.\n",
       "\n",
       "If you're planning a trip, consider going on a guided whale-watching tour, as they can provide the best chances of spotting these magnificent creatures."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37840239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='So you said you were researching ocean mammals?', additional_kwargs={}, response_metadata={}, name='Model'),\n",
       " HumanMessage(content=\"Yes, that's right.\", additional_kwargs={}, response_metadata={}, name='Oliver'),\n",
       " AIMessage(content='Great, what would you like to learn about.', additional_kwargs={}, response_metadata={}, name='Model'),\n",
       " HumanMessage(content='I want to learn about the best place to see Orcas in the US.', additional_kwargs={}, response_metadata={}, name='Oliver')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59f435b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='So you said you were researching ocean mammals?', additional_kwargs={}, response_metadata={}, name='Model'),\n",
       " HumanMessage(content=\"Yes, that's right.\", additional_kwargs={}, response_metadata={}, name='Oliver'),\n",
       " AIMessage(content='Great, what would you like to learn about.', additional_kwargs={}, response_metadata={}, name='Model'),\n",
       " HumanMessage(content='I want to learn about the best place to see Orcas in the US.', additional_kwargs={}, response_metadata={}, name='Oliver'),\n",
       " AIMessage(content=\"One of the best places to see orcas in the U.S. is the San Juan Islands in Washington State. The waters around these islands are known for their resident orca pods, particularly the Southern Resident Killer Whales. The best time to see them is typically from late spring to early fall, especially in June through September.\\n\\nAnother excellent location is the waters off the coast of Alaska, where you can find transient orcas that hunt for seals and other marine mammals. Places like Kenai Fjords National Park and Glacier Bay National Park are popular for orca sightings.\\n\\nAdditionally, the waters around Monterey Bay in California can also be a good spot to see orcas, especially during the migration seasons when they follow their prey.\\n\\nIf you're planning a trip, consider going on a guided whale-watching tour, as they can provide the best chances of spotting these magnificent creatures.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 174, 'prompt_tokens': 67, 'total_tokens': 241, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'id': 'chatcmpl-CBJvvt2D7ZaX6185dzBcfSyuRoldw', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--62956847-2e46-4f98-aa22-91e7a0981a13-0', usage_metadata={'input_tokens': 67, 'output_tokens': 174, 'total_tokens': 241, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='What did we discuss so far?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.append(response)\n",
    "\n",
    "prompt = \"\"\"What did we discuss so far?\"\"\"\n",
    "new_messages = HumanMessage(content=prompt) # or (\"user\", prompt)\n",
    "\n",
    "messages.append(new_messages)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83217ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"So far, we've discussed your interest in ocean mammals, specifically orcas. I provided information about some of the best places in the U.S. to see orcas, including the San Juan Islands in Washington State, Alaska's coastal waters, and Monterey Bay in California. If you have more questions or need further information, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 256, 'total_tokens': 325, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CBJvzeHshbX2Ynv4jxbwobdeYlzQf', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--bff22e31-ca82-4e26-b094-c7995cbd85b1-0', usage_metadata={'input_tokens': 256, 'output_tokens': 69, 'total_tokens': 325, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d9ff05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96060ee0",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "\n",
    "- PromptTemplate\n",
    "- ChatPromptTemplate\n",
    "- MessagesPlaceholder\n",
    "- FewShotPromptTemplate\n",
    "- MessageTemplate\n",
    "- PartialPrompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d69057",
   "metadata": {},
   "source": [
    "#### PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ed804ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the dolphin bring a towel to the party?\\n\\nBecause it wanted to have a whale of a time without getting wet!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 15, 'total_tokens': 40, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CBJw1gLiVAMcT96ZcWUVupWaK5Tcq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--fd36b338-ee7b-426f-bf70-4d6d0a9f7a80-0', usage_metadata={'input_tokens': 15, 'output_tokens': 25, 'total_tokens': 40, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}.\")\n",
    "\n",
    "prompt = prompt_template.invoke({\"topic\": \"ocean mammals\"})\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b2209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77553213",
   "metadata": {},
   "source": [
    "#### ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dabaf012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Orcas, also known as killer whales, are large marine mammals that belong to the dolphin family. They are easily recognized by their black and white coloring, with a distinctive white patch above and behind their eyes. Orcas are very social animals and often live in groups called pods, which can consist of a few individuals to over 40.\\n\\nOrcas are known for their intelligence and complex social structures. They are also skilled hunters and have a varied diet that can include fish, seals, and even other whales. They communicate with each other using a range of sounds, including clicks and whistles.\\n\\nOrcas are found in oceans all around the world, from the Arctic to the Antarctic, and they play an important role in the marine ecosystem. Despite their name, they are not actually whales but are the largest members of the dolphin family.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 166, 'prompt_tokens': 27, 'total_tokens': 193, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CBJw9V88QbvH0XqP3r7XQRv4WjJNb', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--78af7dc3-329a-4809-b000-5f160000c1cb-0', usage_metadata={'input_tokens': 27, 'output_tokens': 166, 'total_tokens': 193, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate([\n",
    "    ('system', 'You are a helpful {domain} expert'),\n",
    "    ('human', 'Explain in simple terms, what is {topic}')\n",
    "])\n",
    "\n",
    "prompt = chat_template.invoke({\"domain\": \"ocean mammals\", \"topic\": \"orcas\"})\n",
    "response = llm.invoke(prompt)\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf7f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ac07845",
   "metadata": {},
   "source": [
    "#### MessagePlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74ede300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can expect your order to be delivered within 3-5 business days from the shipping date. If you have a tracking number, you can use it to get more specific updates on the delivery status. If you need further assistance, feel free to ask!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\"I want to know the status of my latest order.\"),\n",
    "    AIMessage(content=\"Your current order #435 is prepared for shipping. It will be delivered in 3-5 business days.\")\n",
    "]\n",
    "\n",
    "chat_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful customer support agent.\"),\n",
    "    MessagesPlaceholder(variable_name='chat_history'),\n",
    "    (\"human\", \"{query}\")\n",
    "])\n",
    "\n",
    "prompt = chat_template.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"query\": \"When can I expect my order?\"\n",
    "    })\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e34a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96f23b72",
   "metadata": {},
   "source": [
    "#### FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d278e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"}\n",
    "]\n",
    "\n",
    "example_template = PromptTemplate.from_template(\"Word: {word}\\nAntonym: {antonym}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_template,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Word: {input}\\nAntonym:\",\n",
    "    input_variables=[\"input\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n",
    "\n",
    "formatted_prompt = prompt.format(input=\"big\")\n",
    "output = llm.invoke(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a8c0070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      " Give the antonym of every input\n",
      "\n",
      "Word: happy\n",
      "Antonym: sad\n",
      "\n",
      "Word: tall\n",
      "Antonym: short\n",
      "\n",
      "Word: big\n",
      "Antonym:\n",
      "--------------------------------------------------\n",
      "Output: small\n"
     ]
    }
   ],
   "source": [
    "print(\"Prompt:\\n\", formatted_prompt)\n",
    "print(\"-\"*50)\n",
    "print(\"Output:\", output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd92075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87832442",
   "metadata": {},
   "source": [
    "#### MessageTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "481b0109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "898b2271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptTemplate(input_variables=['context', 'field', 'problem', 'years'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['field', 'years'], input_types={}, partial_variables={}, template='You are an expert {field} consultant with {years} years of experience.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'problem'], input_types={}, partial_variables={}, template='I need help with {problem}. My current situation is: {context}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['problem'], input_types={}, partial_variables={}, template='I understand you need help with {problem}. Let me analyze your situation.'), additional_kwargs={})])\n"
     ]
    }
   ],
   "source": [
    "# System message for context\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an expert {field} consultant with {years} years of experience.\"\n",
    ")\n",
    "\n",
    "# Human message for the query\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"I need help with {problem}. My current situation is: {context}\"\n",
    ")\n",
    "\n",
    "# AI message for providing examples of responses\n",
    "ai_prompt = AIMessagePromptTemplate.from_template(\n",
    "    \"I understand you need help with {problem}. Let me analyze your situation.\"\n",
    ")\n",
    "\n",
    "# Combine all messages\n",
    "full_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    human_prompt,\n",
    "    ai_prompt\n",
    "])\n",
    "\n",
    "pprint(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adbf29db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"MESSAGES: [SystemMessage(content='You are an expert marketing consultant \"\n",
      " \"with 10 years of experience.', additional_kwargs={}, response_metadata={}), \"\n",
      " 'HumanMessage(content=\"I need help with low conversion rates on my website. '\n",
      " \"My current situation is: We're getting a lot of traffic, but very few \"\n",
      " 'visitors are signing up for our product.\", additional_kwargs={}, '\n",
      " \"response_metadata={}), AIMessage(content='I understand you need help with \"\n",
      " \"low conversion rates on my website. Let me analyze your situation.', \"\n",
      " 'additional_kwargs={}, response_metadata={})]')\n"
     ]
    }
   ],
   "source": [
    "# Provide input values for all placeholders\n",
    "formatted_prompt = full_prompt.invoke({\n",
    "    \"field\": \"marketing\",\n",
    "    \"years\": \"10\",\n",
    "    \"problem\": \"low conversion rates on my website\",\n",
    "    \"context\": \"We're getting a lot of traffic, but very few visitors are signing up for our product.\"\n",
    "})\n",
    "\n",
    "# Print each message from the formatted prompt\n",
    "for role, content in formatted_prompt:\n",
    "    pprint(f\"{role.upper()}: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5a007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58161832",
   "metadata": {},
   "source": [
    "#### Partial Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8db104d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a customer service helping with order processing in e-commerce.\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"You are a {role} helping with {task} in {domain}.\",\n",
    "    input_variables=[\"role\", \"task\", \"domain\"]\n",
    ")\n",
    "\n",
    "# Partially fill in the template\n",
    "partial_prompt = prompt.partial(role=\"customer service\")\n",
    "\n",
    "# Later, complete the remaining variables\n",
    "final_prompt = partial_prompt.format(\n",
    "    task=\"order processing\",\n",
    "    domain=\"e-commerce\"\n",
    ")\n",
    "\n",
    "# Output the final prompt\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27602ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f744bd80",
   "metadata": {},
   "source": [
    "### Prompting Strategies\n",
    "\n",
    "- Zero-Shot Prompting\n",
    "- Few-Shot Prompting\n",
    "- Chain-of-Thought (CoT) Prompting\n",
    "    - Zero-shot-CoT\n",
    "    - Few-shot-CoT\n",
    "-  Tree-of-Thought (ToT) Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6556e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c4fd1",
   "metadata": {},
   "source": [
    "#### Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98ac692b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of \"How are you?\" in French is \"Comment ça va ?\" or simply \"Ça va ?\" for a more informal version.\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"Translate to French: {sentence}\")\n",
    "response = llm.invoke(prompt.format(sentence=\"How are you?\"))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe800c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09558e44",
   "metadata": {},
   "source": [
    "#### Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32092940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merci\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Translate English to French:\n",
    "- Hello → Bonjour\n",
    "- Good night → Bonne nuit\n",
    "- {text} →\"\"\")\n",
    "\n",
    "response = llm.invoke(prompt.format(text=\"Thank you\"))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780b173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efe4ae50",
   "metadata": {},
   "source": [
    "#### Zero-Shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08c1af47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find out how many cakes are left, we can follow these steps:\n",
      "\n",
      "1. **Calculate the total number of cakes made**:\n",
      "   - Cakes made in the morning: 20\n",
      "   - Cakes made in the afternoon: 35\n",
      "   - Total cakes made = 20 + 35 = 55 cakes\n",
      "\n",
      "2. **Determine how many cakes were sold**:\n",
      "   - Total cakes sold = 40 cakes\n",
      "\n",
      "3. **Calculate the number of cakes left**:\n",
      "   - Cakes left = Total cakes made - Total cakes sold\n",
      "   - Cakes left = 55 - 40 = 15 cakes\n",
      "\n",
      "So, the number of cakes left is **15 cakes**.\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Solve the following problem.\n",
    "\n",
    "Question: A bakery made 20 cakes in the morning and 35 cakes in the afternoon. They sold 40 cakes in total.\n",
    "How many cakes are left?\n",
    "\n",
    "Answer: Let's think step by step.\n",
    "\"\"\")\n",
    "\n",
    "response = llm.invoke(prompt.format())\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2695fa83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b87668b",
   "metadata": {},
   "source": [
    "#### Few-Shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83c639b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Start with 120 books.  \n",
      "Step 2: Subtract the borrowed books → 120 - 35 = 85  \n",
      "Step 3: Add the new books → 85 + 20 = 105  \n",
      "Answer: 105 books are in the library now.\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Solve each of the following problems step-by-step.\n",
    "\n",
    "Q: Mike had 8 oranges. He ate 2 and bought 5 more. How many oranges does he have now?  \n",
    "A: Step 1: Start with 8.  \n",
    "Step 2: Subtract 2 → 8 - 2 = 6  \n",
    "Step 3: Add 5 → 6 + 5 = 11  \n",
    "Answer: 11\n",
    "\n",
    "Q: A library had 120 books. 35 were borrowed, and 20 new books were added. How many books are in the library now?  \n",
    "A:\"\"\")\n",
    "\n",
    "response = llm.invoke(prompt.format())\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a7d547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2f6bda8",
   "metadata": {},
   "source": [
    "#### Tree-of-Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65b1dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3dc97f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the evaluations, **Meal Option 2: Baked Salmon with Sweet Potato and Spinach** is the best meal option. \n",
      "\n",
      "This choice is justified by its exceptional nutritional profile, particularly the high omega-3 fatty acids from salmon, which are beneficial for heart health. The combination of sweet potatoes and spinach not only provides complex carbohydrates and essential vitamins but also ensures a well-rounded meal that is both filling and nutritious. Additionally, the preparation time is relatively quick, making it suitable for those with a busy schedule. While it is slightly more expensive than the other options, the health benefits and quality of ingredients make it a worthwhile investment for a wholesome meal.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate Meal Options\n",
    "prompt1 = PromptTemplate.from_template(\"\"\"\n",
    "You're helping someone plan a healthy meal.\n",
    "\n",
    "Step 1:\n",
    "Come up with three different meal options. Be practical and consider variety, nutritional balance, and taste.\n",
    "\n",
    "A:\"\"\")\n",
    "\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt1, output_key=\"meal_ideas\")\n",
    "\n",
    "\n",
    "# Step 2: Evaluate Each Meal\n",
    "prompt2 = PromptTemplate.from_template(\"\"\"\n",
    "Step 2:\n",
    "Evaluate the pros and cons of each of the following meal options:\n",
    "\n",
    "{meal_ideas}\n",
    "\n",
    "Include nutritional value, preparation time, and cost in your evaluation.\n",
    "\n",
    "A:\"\"\")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2, output_key=\"meal_evaluations\")\n",
    "\n",
    "\n",
    "# Step 3: Choose the Best Meal\n",
    "prompt3 = PromptTemplate.from_template(\"\"\"\n",
    "Step 3:\n",
    "Based on the evaluations, pick the best meal option. Justify your choice in a few sentences.\n",
    "\n",
    "{meal_evaluations}\n",
    "\n",
    "A:\"\"\")\n",
    "chain3 = LLMChain(llm=llm, prompt=prompt3, output_key=\"final_meal_choice\")\n",
    "\n",
    "\n",
    "# Create the overall SequentialChain\n",
    "meal_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3],\n",
    "    input_variables=[],\n",
    "    output_variables=[\"final_meal_choice\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "response = meal_chain.invoke({})\n",
    "print(response[\"final_meal_choice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547a17ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "538eece3",
   "metadata": {},
   "source": [
    "### Structured Output & Output Parsers\n",
    "\n",
    "- TypedDict\n",
    "- Pydantic\n",
    "- JSON schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af2986df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "class Person(TypedDict):\n",
    "    name: str\n",
    "    age: int\n",
    "    email: str\n",
    "    \n",
    "    \n",
    "person_data: Person = {\n",
    "    \"name\": \"Oliver\",\n",
    "    \"age\": 28,\n",
    "    \"email\": \"oliver@me.com\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b5fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9bd20b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Oliver', 'age': 28, 'email': 'oliver@me.com', 'cgpa': 5}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, EmailStr, Field\n",
    "from typing import Optional\n",
    "\n",
    "class Student(BaseModel):\n",
    "    name: str\n",
    "    age: Optional[int] = None\n",
    "    email: EmailStr\n",
    "    cgpa: float = Field(gt=0, lt=10, default=5, description='A decimal value representing the cgpa of the student')\n",
    "    \n",
    "    \n",
    "new_student = {'name': 'Oliver', 'age': 28, 'email': 'oliver@me.com'}\n",
    "student = Student(**new_student)\n",
    "student_dict = dict(student)\n",
    "print(student_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda4d103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47bc700a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'object',\n",
       " 'properties': {'name': {'type': 'string'},\n",
       "  'price': {'type': 'number'},\n",
       "  'in_stock': {'type': 'boolean'}},\n",
       " 'required': ['name', 'price', 'in_stock']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"name\": {\"type\": \"string\"},\n",
    "    \"price\": {\"type\": \"number\"},\n",
    "    \"in_stock\": {\"type\": \"boolean\"}\n",
    "  },\n",
    "  \"required\": [\"name\", \"price\", \"in_stock\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4086b858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "114a2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain_core.output_parsers import (\n",
    "    StrOutputParser,\n",
    "    JsonOutputParser,\n",
    "    PydanticOutputParser\n",
    ")\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "58276d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ocean mammals are warm-blooded, air-breathing animals that live in marine environments, including species such as whales, dolphins, and seals, and are characterized by their adaptations for life in water, such as streamlined bodies and specialized respiratory systems.\n"
     ]
    }
   ],
   "source": [
    "parser = StrOutputParser()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Explain {topic} in one sentence.\")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "response = chain.invoke({\"topic\": \"ocean mammals\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd2097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8ea26052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'facts': [{'fact': \"Baseball is often referred to as America's pastime and has a rich history dating back to the 19th century.\"}, {'fact': 'The longest professional baseball game in history lasted 33 innings and took place in 1981 between the Pawtucket Red Sox and the Rochester Red Wings.'}]}\n"
     ]
    }
   ],
   "source": [
    "parser = JsonOutputParser()\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template=\"Give me 2 facts about {topic} \\n{format_instructions}\",\n",
    "    input_variables=[\"topic\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chat_template = template | llm | parser\n",
    "\n",
    "response = chat_template.invoke({\"topic\": \"Baseball\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc5111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0febbe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'habit_1': 'Drink at least 8 glasses of water daily to stay hydrated.', 'habit_2': 'Incorporate at least 30 minutes of physical activity into your daily routine.'}\n"
     ]
    }
   ],
   "source": [
    "schema = [\n",
    "    ResponseSchema(name='habit_1', description='First healthy habit to follow'),\n",
    "    ResponseSchema(name='habit_2', description='Second healthy habit to follow'),\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schema)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template=\"Give me 2 healthy habits \\n{format_instructions}\",\n",
    "    input_variables=[\"topic\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chat_template = template | llm | parser\n",
    "\n",
    "response = chat_template.invoke({})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294fe05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6a1670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='The Notebook' genre='romance' release_year=2004 why_recommended='A timeless love story that beautifully captures the essence of enduring romance and the power of memory.'\n"
     ]
    }
   ],
   "source": [
    "class MovieRecommendation(BaseModel):\n",
    "    title: str = Field(description=\"Name of the movie\")\n",
    "    genre: Literal[\"action\", \"comedy\", \"drama\", \"sci-fi\", \"romance\"] = Field(description=\"Main genre of the movie\")\n",
    "    release_year: int = Field(description=\"Release year of the movie\")\n",
    "    why_recommended: str = Field(description=\"Short reason why this movie is recommended\")\n",
    "    \n",
    "parser = PydanticOutputParser(pydantic_object=MovieRecommendation)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Recommend one {genre} movie for tonight. Format as:\\n{format_instructions}\",\n",
    "    input_variables=[\"genre\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "response = chain.invoke({\"genre\": \"romance\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a0bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "827b1b44",
   "metadata": {},
   "source": [
    "### LCEL: The Runnable Architecture | Chain\n",
    "\n",
    "LCEL = Composition syntax.\n",
    "\n",
    "Runnables = The standard protocol that powers it all.\n",
    "\n",
    "- PromptTemplate | LLM | OutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764224d4",
   "metadata": {},
   "source": [
    "#### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "45a18c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\"Zippy's Starry Quest: A Journey of Curiosity\"**\n"
     ]
    }
   ],
   "source": [
    "parser = StrOutputParser()\n",
    "\n",
    "# Step 1: Generate a short story\n",
    "prompt1 = PromptTemplate(\n",
    "    template=\"Write a short bedtime story about {character} and their adventure.\",\n",
    "    input_variables=[\"character\"]\n",
    ")\n",
    "\n",
    "# Step 2: Extract a good title for the story\n",
    "prompt2 = PromptTemplate(\n",
    "    template=\"Here is a story:\\n\\n{text}\\n\\nSuggest a creative and catchy title for it:\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Chain: story -> title\n",
    "chain = prompt1 | llm | parser | prompt2 | llm | parser\n",
    "\n",
    "# Run\n",
    "result = chain.invoke({\"character\": \"a curious little robot\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b65774e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +------------+       \n",
      "      | ChatOpenAI |       \n",
      "      +------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +------------+       \n",
      "      | ChatOpenAI |       \n",
      "      +------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644445f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c26c0498",
   "metadata": {},
   "source": [
    "#### Parallel Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3519a392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a great summary and a clever joke! AI indeed mimics human intelligence, allowing machines to tackle complex tasks. And the joke about the robot going on a diet is a fun play on words with \"bytes\"—a perfect blend of technology and humor! If you have more questions or need further information about AI, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Prompt 1: Summary\n",
    "summary_prompt = PromptTemplate.from_template(\"Summarize {topic} in 1 sentence.\")\n",
    "summary_chain = summary_prompt | llm | parser\n",
    "\n",
    "# Prompt 2: Joke\n",
    "joke_prompt = PromptTemplate.from_template(\"Tell a short joke about {topic}.\")\n",
    "joke_chain = joke_prompt | llm | parser\n",
    "\n",
    "# Run both in parallel\n",
    "parallel = RunnableParallel({\n",
    "    \"summary\": summary_chain,\n",
    "    \"joke\": joke_chain\n",
    "})\n",
    "\n",
    "# Merge both outputs\n",
    "merge_prompt = PromptTemplate.from_template(\"Summary: {summary}\\nJoke: {joke}\")\n",
    "merge_chain = merge_prompt | llm | parser\n",
    "\n",
    "# Final chain\n",
    "chain = parallel | merge_chain\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"topic\": \"Artificial Intelligence\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1f2eb9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          +-----------------------------+            \n",
      "          | Parallel<summary,joke>Input |            \n",
      "          +-----------------------------+            \n",
      "                 **               **                 \n",
      "              ***                   ***              \n",
      "            **                         **            \n",
      "+----------------+                +----------------+ \n",
      "| PromptTemplate |                | PromptTemplate | \n",
      "+----------------+                +----------------+ \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "  +------------+                    +------------+   \n",
      "  | ChatOpenAI |                    | ChatOpenAI |   \n",
      "  +------------+                    +------------+   \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "+-----------------+              +-----------------+ \n",
      "| StrOutputParser |              | StrOutputParser | \n",
      "+-----------------+              +-----------------+ \n",
      "                 **               **                 \n",
      "                   ***         ***                   \n",
      "                      **     **                      \n",
      "          +------------------------------+           \n",
      "          | Parallel<summary,joke>Output |           \n",
      "          +------------------------------+           \n",
      "                          *                          \n",
      "                          *                          \n",
      "                          *                          \n",
      "                 +----------------+                  \n",
      "                 | PromptTemplate |                  \n",
      "                 +----------------+                  \n",
      "                          *                          \n",
      "                          *                          \n",
      "                          *                          \n",
      "                   +------------+                    \n",
      "                   | ChatOpenAI |                    \n",
      "                   +------------+                    \n",
      "                          *                          \n",
      "                          *                          \n",
      "                          *                          \n",
      "                +-----------------+                  \n",
      "                | StrOutputParser |                  \n",
      "                +-----------------+                  \n",
      "                          *                          \n",
      "                          *                          \n",
      "                          *                          \n",
      "              +-----------------------+              \n",
      "              | StrOutputParserOutput |              \n",
      "              +-----------------------+              \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba8390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a3042b4",
   "metadata": {},
   "source": [
    "#### Conditional Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b461b5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How about going for a picnic in a nearby park? You can pack some delicious snacks, bring a blanket, and enjoy the sunshine. You could also bring along a frisbee or a ball for some fun games, or simply relax and read a book in the fresh air. If you're feeling adventurous, consider a hike or a bike ride to explore the area!\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import  RunnableBranch, RunnableLambda\n",
    "\n",
    "class Weather(BaseModel):\n",
    "    condition: Literal[\"sunny\", \"rainy\"] = Field(description=\"The weather condition\")\n",
    "\n",
    "weather_parser = PydanticOutputParser(pydantic_object=Weather)\n",
    "text_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "# Weather classification prompt\n",
    "weather_prompt = PromptTemplate(\n",
    "    template=\"Classify the weather as 'sunny' or 'rainy': {forecast} \\n{format_instruction}\",\n",
    "    input_variables=[\"forecast\"],\n",
    "    partial_variables={\"format_instruction\": weather_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "classifier_chain = weather_prompt | llm | weather_parser\n",
    "\n",
    "# Branches for advice\n",
    "sunny_prompt = PromptTemplate.from_template(\"It's sunny! Suggest a fun outdoor activity.\")\n",
    "sunny_chain = sunny_prompt | llm | StrOutputParser()\n",
    "\n",
    "rainy_prompt = PromptTemplate.from_template(\"It's rainy! Suggest a cozy indoor activity.\")\n",
    "rainy_chain = rainy_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Branching logic\n",
    "branch = RunnableBranch(\n",
    "    # unwrap Weather → {}\n",
    "    (lambda x: x.condition == \"sunny\", RunnableLambda(lambda _: {}) | sunny_chain),\n",
    "    (lambda x: x.condition == \"rainy\", RunnableLambda(lambda _: {}) | rainy_chain),\n",
    "    RunnableLambda(lambda _: \"Could not classify the weather.\")\n",
    ")\n",
    "\n",
    "chain = classifier_chain | branch\n",
    "\n",
    "forecast_text = \"Today the sky is clear with bright sunshine.\"\n",
    "print(chain.invoke({\"forecast\": forecast_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d94df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02add2ce",
   "metadata": {},
   "source": [
    "#### Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e110f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()\n",
    "\n",
    "# Math expert\n",
    "math_prompt = PromptTemplate.from_template(\n",
    "    \"You are a math tutor. Solve this math problem step by step:\\n{question}\"\n",
    ")\n",
    "math_chain = math_prompt | llm | parser\n",
    "\n",
    "# History expert\n",
    "history_prompt = PromptTemplate.from_template(\n",
    "    \"You are a history teacher. Explain this historical question in detail:\\n{question}\"\n",
    ")\n",
    "history_chain = history_prompt | llm | parser\n",
    "\n",
    "# Router Logic\n",
    "def route_question(input):\n",
    "    q = input[\"question\"].lower()\n",
    "    if any(word in q for word in [\"add\", \"subtract\", \"multiply\", \"divide\", \"math\", \"number\"]):\n",
    "        return 0  # Math\n",
    "    elif \"history\" in q or \"war\" in q or \"year\" in q or \"president\" in q:\n",
    "        return 1  # History\n",
    "    else:\n",
    "        return 2  # Fallback\n",
    "    \n",
    "# Fallback response\n",
    "fallback_chain = RunnableLambda(\n",
    "    lambda x: \"Sorry, I can only answer Math or History questions right now.\"\n",
    ")\n",
    "\n",
    "# Router chain\n",
    "router_chain = RunnableBranch(\n",
    "    (lambda x: route_question(x) == 0, math_chain),\n",
    "    (lambda x: route_question(x) == 1, history_chain),\n",
    "    fallback_chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "99073c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the problem of multiplying 12 by 7, we can follow these steps:\n",
      "\n",
      "1. **Write the multiplication expression**: \n",
      "   \\[\n",
      "   12 \\times 7\n",
      "   \\]\n",
      "\n",
      "2. **Break down the multiplication**: \n",
      "   We can think of 12 as \\(10 + 2\\). So, we can use the distributive property:\n",
      "   \\[\n",
      "   12 \\times 7 = (10 + 2) \\times 7\n",
      "   \\]\n",
      "\n",
      "3. **Distribute 7 to both terms**:\n",
      "   \\[\n",
      "   = 10 \\times 7 + 2 \\times 7\n",
      "   \\]\n",
      "\n",
      "4. **Calculate each multiplication**:\n",
      "   - For \\(10 \\times 7\\):\n",
      "     \\[\n",
      "     10 \\times 7 = 70\n",
      "     \\]\n",
      "   - For \\(2 \\times 7\\):\n",
      "     \\[\n",
      "     2 \\times 7 = 14\n",
      "     \\]\n",
      "\n",
      "5. **Add the results together**:\n",
      "   \\[\n",
      "   70 + 14 = 84\n",
      "   \\]\n",
      "\n",
      "6. **Final answer**:\n",
      "   \\[\n",
      "   12 \\times 7 = 84\n",
      "   \\]\n",
      "\n",
      "So, \\(12\\) multiplied by \\(7\\) equals \\(84\\).\n"
     ]
    }
   ],
   "source": [
    "print(router_chain.invoke({\"question\": \"What is 12 multiply by 7?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6d267d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first president of the United States was George Washington, who served from April 30, 1789, to March 4, 1797. His presidency marked the beginning of the new federal government established by the Constitution, which was ratified in 1788.\n",
      "\n",
      "### Background\n",
      "\n",
      "George Washington was born on February 22, 1732, in Westmoreland County, Virginia. He grew up in a plantation family and received a basic education. Washington began his career as a surveyor and gained valuable experience in the Virginia wilderness. His military career began during the French and Indian War (1754-1763), where he served as a lieutenant colonel in the Virginia militia.\n",
      "\n",
      "### Role in the American Revolution\n",
      "\n",
      "Washington's most significant contribution to American history came during the American Revolutionary War (1775-1783). He was appointed as the commander-in-chief of the Continental Army by the Second Continental Congress in 1775. Washington faced numerous challenges, including a lack of resources, training, and support. However, his leadership, strategic acumen, and ability to inspire his troops were crucial in securing American independence from British rule.\n",
      "\n",
      "One of his most notable achievements was the surprise attack on the Hessian forces in Trenton, New Jersey, on December 26, 1776, which revitalized the American cause. Washington's leadership culminated in the victory at the Siege of Yorktown in 1781, effectively ending the war.\n",
      "\n",
      "### The Constitutional Convention\n",
      "\n",
      "After the war, Washington's reputation as a unifying figure led to his involvement in the Constitutional Convention of 1787. He was elected president of the convention, where delegates drafted the United States Constitution. Washington's support for the new Constitution was pivotal in its ratification, as he was widely respected and trusted by the American public.\n",
      "\n",
      "### Presidency\n",
      "\n",
      "In 1789, Washington was unanimously elected as the first president of the United States. He set many precedents that would shape the office and the federal government. Washington established the tradition of a two-term limit for presidents, which was later codified in the 22nd Amendment in 1951. He also created a cabinet of advisors, which included figures like Thomas Jefferson (Secretary of State) and Alexander Hamilton (Secretary of the Treasury).\n",
      "\n",
      "During his presidency, Washington faced several significant issues, including:\n",
      "\n",
      "1. **Establishing a New Government**: Washington worked to implement the new Constitution and establish the federal government’s authority. He emphasized the importance of a strong central government while balancing the rights of states.\n",
      "\n",
      "2. **Foreign Policy**: Washington advocated for neutrality in foreign conflicts, particularly in the ongoing wars between Britain and France. His Farewell Address in 1796 warned against entangling alliances and political parties, which he believed could divide the nation.\n",
      "\n",
      "3. **Domestic Challenges**: His administration dealt with various domestic issues, including the Whiskey Rebellion in 1794, which tested the federal government's ability to enforce its laws.\n",
      "\n",
      "### Legacy\n",
      "\n",
      "George Washington's presidency laid the foundation for the future of the United States. He is often referred to as the \"Father of His Country\" for his leadership during the Revolutionary War and his role in the founding of the nation. His commitment to democratic principles, civic virtue, and national unity has made him an enduring symbol of American ideals.\n",
      "\n",
      "Washington declined to seek a third term, setting a precedent for future presidents and reinforcing the principle of voluntary transfer of power. He passed away on December 14, 1799, but his legacy continues to influence American politics and governance to this day.\n"
     ]
    }
   ],
   "source": [
    "print(router_chain.invoke({\"question\": \"Who was the first president of the United States?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "04d71bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I can only answer Math or History questions right now.\n"
     ]
    }
   ],
   "source": [
    "print(router_chain.invoke({\"question\": \"Tell me about football.\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb8ddcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
